{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a962959",
   "metadata": {},
   "source": [
    "# DAEN 429 Final Project\n",
    "Sydney Flake, Maddie Bird, Jade Winebright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3ce65",
   "metadata": {},
   "source": [
    "## Phase 0: Setup + ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c049e893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # use GPU if available\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 429\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Turn on cuDNN benchmark for speed (optional)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f121953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 11534336 bytes (1089352698 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/grassknoted/asl-alphabet?dataset_version_number=1 (11534336/1100887034) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.03G/1.03G [03:12<00:00, 5.65MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/flake/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "#path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "123ea263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations for training and validation sets\n",
    "\n",
    "# ImageNet normalization (what ResNet-18 expects)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00fcb963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 87000\n",
      "Number of classes: 29\n",
      "Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'] ...\n"
     ]
    }
   ],
   "source": [
    "# Load the full ASL dataset\n",
    "\n",
    "DATA_ROOT = \"/Users/flake/Documents/DAEN429/project/Datasets/asl_alphabet_train/asl_alphabet_train\"\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=DATA_ROOT, transform=None)\n",
    "print(\"Total images:\", len(full_dataset))\n",
    "print(\"Number of classes:\", len(full_dataset.classes))\n",
    "print(\"Classes:\", full_dataset.classes[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a3682f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (indices): 69600\n",
      "Val size (indices): 17400\n"
     ]
    }
   ],
   "source": [
    "# ---- Stratified 80/20 split with seed = 429 ----\n",
    "indices = np.arange(len(full_dataset))\n",
    "labels = np.array(full_dataset.targets)  # class indices 0..(num_classes-1)\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=429\n",
    ")\n",
    "\n",
    "print(\"Train size (indices):\", len(train_idx))\n",
    "print(\"Val size (indices):\", len(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "536c8cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 69600\n",
      "Val size: 17400\n"
     ]
    }
   ],
   "source": [
    "# ---- Subset wrapper that allows per-split transforms ----\n",
    "class SubsetWithTransform(Subset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        img, label = self.dataset[real_idx]  # base dataset has transform=None\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Make sure base dataset doesn’t apply transforms itself\n",
    "full_dataset.transform = None\n",
    "\n",
    "train_dataset = SubsetWithTransform(full_dataset, train_idx, transform=train_transform)\n",
    "val_dataset   = SubsetWithTransform(full_dataset, val_idx,   transform=val_transform)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198e060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train batches: 1088\n",
      "Num val batches: 272\n"
     ]
    }
   ],
   "source": [
    "# ---- DataLoaders ----\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "pin = True if device.type == \"cuda\" else False\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=pin,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=pin,\n",
    ")\n",
    "\n",
    "print(\"Num train batches:\", len(train_loader))\n",
    "print(\"Num val batches:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118b64f",
   "metadata": {},
   "source": [
    "## Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd4b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 29\n",
      "Classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(full_dataset.classes)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Classes:\", full_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0182548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet18_model(num_classes, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create a ResNet-18 model with a custom classifier head for ASL classes.\n",
    "    If pretrained=True → use ImageNet weights (for T-A, T-B, T-C).\n",
    "    If pretrained=False → random init (for S-A).\n",
    "    \"\"\"\n",
    "    if pretrained:\n",
    "        weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "        model = resnet18(weights=weights)\n",
    "        print(\"Loaded ResNet-18 with ImageNet pretrained weights.\")\n",
    "    else:\n",
    "        model = resnet18(weights=None)\n",
    "        print(\"Loaded ResNet-18 from scratch (no pretrained weights).\")\n",
    "    \n",
    "    # Replace the final fully connected layer\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f675676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing/unfreezing policies\n",
    "def apply_freezing_policy(model, policy):\n",
    "    \"\"\"\n",
    "    policy: one of {\"T-A\", \"T-B\", \"T-C\", \"S-A\"}\n",
    "    \n",
    "    T-A: Head-only, freeze all backbone, train fc.\n",
    "    T-B: Freeze stem + layer1 + layer2 + layer3; train layer4 + fc.\n",
    "    T-C: Freeze stem + layer1 + layer2; train layer3 + layer4 + fc.\n",
    "    S-A: From scratch, train all layers (no freezing).\n",
    "    \"\"\"\n",
    "    # First, freeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if policy == \"T-A\":\n",
    "        # Train only the classifier head\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    elif policy == \"T-B\":\n",
    "        # Train layer4 and head\n",
    "        for param in model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    elif policy == \"T-C\":\n",
    "        # Train layer3, layer4, and head\n",
    "        for param in model.layer3.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    elif policy == \"S-A\":\n",
    "        # From scratch: train everything\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown policy: {policy}\")\n",
    "\n",
    "    # Optional: set BatchNorm layers in frozen parts to eval mode\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            # if all params in this BN are frozen, keep it in eval\n",
    "            if not any(p.requires_grad for p in m.parameters()):\n",
    "                m.eval()\n",
    "\n",
    "    # Print a quick summary\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Policy {policy}: trainable params = {trainable_params}/{total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed0cda1",
   "metadata": {},
   "source": [
    "Reusable code for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "384c8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_optimizer(model, lr=1e-3, weight_decay=1e-4, optimizer_name=\"Adam\"):\n",
    "    \"\"\"\n",
    "    Returns an optimizer over ONLY trainable parameters.\n",
    "    \"\"\"\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "    \n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e32f2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "        all_targets.append(labels.detach().cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd6d7cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.append(preds.detach().cpu())\n",
    "            all_targets.append(labels.detach().cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = accuracy_score(all_targets, all_preds)\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1, all_preds, all_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af9940d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    optimizer_name=\"Adam\",\n",
    "    experiment_name=\"exp\"\n",
    "):\n",
    "    optimizer = get_optimizer(model, lr=lr, weight_decay=weight_decay, optimizer_name=optimizer_name)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": [],\n",
    "    }\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_acc, val_f1, _, _ = evaluate(model, val_loader, device)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"train_f1\"].append(train_f1)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"val_f1\": val_f1,\n",
    "                \"val_acc\": val_acc,\n",
    "            }\n",
    "\n",
    "        print(\n",
    "            f\"[{experiment_name}] Epoch {epoch:02d}/{num_epochs:02d} | \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\n[{experiment_name}] Best val macro-F1: {best_val_f1:.4f}\")\n",
    "    return model, history, best_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec448c2",
   "metadata": {},
   "source": [
    "### T-A: Head Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca51348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== T-A: Head-only finetuning =====\n",
    "model_TA = create_resnet18_model(num_classes=num_classes, pretrained=True)\n",
    "apply_freezing_policy(model_TA, policy=\"T-A\")\n",
    "model_TA = model_TA.to(device)\n",
    "\n",
    "model_TA, history_TA, best_TA = train_model(\n",
    "    model_TA,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    num_epochs=10,          # you can adjust\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    optimizer_name=\"Adam\",\n",
    "    experiment_name=\"T-A_head_only\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07873237",
   "metadata": {},
   "source": [
    "### T-B: Last Block Unfrozen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393e559",
   "metadata": {},
   "source": [
    "### T-C: Progressive Unfreezing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbcd277",
   "metadata": {},
   "source": [
    "### S-A: Train From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710094b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (daen429)",
   "language": "python",
   "name": "daen429"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
